[
  {
    "name": "OTEL Demo: Frontend Slow Page Load",
    "description": "Investigate why the Astronomy Shop frontend is slow to render product pages using trace data from the OpenTelemetry demo",
    "category": "RCA",
    "difficulty": "Easy",
    "initialPrompt": "Users of the OpenTelemetry Astronomy Shop report that the product listing page takes over 5 seconds to load. The frontend service is instrumented with OpenTelemetry and traces are stored in OpenSearch. Can you query the otel-v1-apm-span-* index to find which backend call is causing the slowness?",
    "context": [
      {
        "description": "OTEL Demo Services",
        "value": "OpenTelemetry Astronomy Shop services involved in product page load:\n- frontend (Node.js): serves the UI, makes gRPC calls to backend services\n- productcatalogservice (Go): ListProducts, GetProduct\n- recommendationservice (Python): ListRecommendations → calls productcatalogservice\n- adservice (Java): GetAds\n- featureflagservice (Erlang): GetFlag\nAll services emit traces to: otel-v1-apm-span-*"
      },
      {
        "description": "OpenSearch Index Schema",
        "value": "Index: otel-v1-apm-span-*\nKey fields:\n- traceId, spanId, parentSpanId\n- serviceName (resource.service.name)\n- operationName (span.name)\n- duration (durationInNanos)\n- status.code: 0=UNSET, 1=OK, 2=ERROR\n- span.kind: SPAN_KIND_SERVER, SPAN_KIND_CLIENT\n- span.attributes.http.method, span.attributes.http.status_code\n- span.attributes.rpc.method, span.attributes.rpc.service"
      },
      {
        "description": "Symptoms",
        "value": "- frontend spans for / route show duration > 5000ms\n- Normal baseline for product page load is ~300ms\n- No error spans detected — this is a latency issue, not failures\n- The slowness started within the last hour"
      }
    ],
    "expectedOutcomes": [
      "Query otel-v1-apm-span-* for frontend spans with high duration",
      "Identify which downstream gRPC call is the bottleneck by comparing child span durations",
      "Find that productcatalogservice or recommendationservice has elevated latency",
      "Suggest looking at the slow service's spans to find the root cause"
    ]
  },
  {
    "name": "OTEL Demo: Cart Service Redis Connection Errors",
    "description": "Diagnose cartservice failures caused by Redis connectivity issues using OTEL demo traces in OpenSearch",
    "category": "RCA",
    "difficulty": "Easy",
    "initialPrompt": "The OpenTelemetry Astronomy Shop cartservice is returning errors when users try to add items to their cart. The error rate went from 0% to 35% in the last 15 minutes. Investigate the traces in the otel-v1-apm-span-* index to determine the root cause.",
    "context": [
      {
        "description": "Cart Service Architecture",
        "value": "cartservice (C#/.NET):\n- Exposes gRPC API: AddItem, GetCart, EmptyCart\n- Backed by Redis (valkey-cart) for session storage\n- Called by: frontend, checkoutservice\n- Traces in: otel-v1-apm-span-*\n- Redis spans appear as CLIENT spans with db.system=redis"
      },
      {
        "description": "Relevant Span Attributes",
        "value": "For cartservice spans:\n- serviceName: cartservice\n- span.attributes.rpc.service: oteldemo.CartService\n- span.attributes.rpc.method: AddItem, GetCart, EmptyCart\n\nFor Redis spans:\n- span.attributes.db.system: redis\n- span.attributes.db.statement: SET, GET, DEL\n- span.attributes.net.peer.name: valkey-cart\n- status.code: 2 when connection fails"
      },
      {
        "description": "Error Pattern",
        "value": "- 35% of cartservice spans have status.code = 2\n- Error spans contain event with name 'exception'\n- exception.message shows 'Redis connection refused' or 'timeout'\n- Affected operations: AddItem, GetCart\n- EmptyCart calls appear unaffected (cached locally)"
      }
    ],
    "expectedOutcomes": [
      "Query otel-v1-apm-span-* for cartservice error spans with status.code = 2",
      "Find Redis CLIENT spans with connection errors in child spans",
      "Identify that Redis (valkey-cart) is the root cause — connection refused or timeout",
      "Recommend checking Redis pod health, network connectivity, or resource limits"
    ]
  },
  {
    "name": "OTEL Demo: Checkout Flow Cascade Failure",
    "description": "Trace a cascading failure through the checkout flow from paymentservice to frontend using OTEL demo data",
    "category": "RCA",
    "difficulty": "Medium",
    "initialPrompt": "Customers of the Astronomy Shop cannot complete purchases. The checkout button shows an error after a long wait. Multiple services appear affected. Use the distributed traces in the otel-v1-apm-span-* index to trace the full checkout request lifecycle and identify where the failure originates.",
    "context": [
      {
        "description": "Checkout Flow",
        "value": "Checkout request path (all gRPC):\n1. frontend → checkoutservice.PlaceOrder\n2. checkoutservice → cartservice.GetCart\n3. checkoutservice → productcatalogservice.GetProduct (for each item)\n4. checkoutservice → currencyservice.Convert\n5. checkoutservice → shippingservice.GetQuote / ShipOrder\n6. checkoutservice → paymentservice.Charge\n7. checkoutservice → emailservice.SendOrderConfirmation\n8. checkoutservice → cartservice.EmptyCart"
      },
      {
        "description": "OpenSearch Query Guide",
        "value": "Useful queries against otel-v1-apm-span-*:\n- Error spans: status.code = 2\n- Checkout traces: filter by serviceName='checkoutservice' AND span.attributes.rpc.method='PlaceOrder'\n- Follow a trace: use traceId to get all spans, sort by startTime\n- Service error rates: aggregate by serviceName, count where status.code = 2\n- Span events contain exception details: events.name = 'exception'"
      },
      {
        "description": "Current Error State",
        "value": "Error rates from traces (last 30 min):\n- frontend: 40% on /api/checkout\n- checkoutservice: 60% on PlaceOrder\n- paymentservice: 95% on Charge\n- All other services: < 1% error rate\n- paymentservice spans show duration = 30s (timeout) before error\n- checkoutservice retries paymentservice.Charge up to 3 times"
      }
    ],
    "expectedOutcomes": [
      "Query traces for checkoutservice.PlaceOrder errors and follow the trace tree",
      "Identify paymentservice.Charge as the originating failure with 95% error rate",
      "Show the cascade: paymentservice timeout → checkoutservice retry exhaustion → frontend error",
      "Find evidence of retries amplifying the problem (3 attempts per checkout)",
      "Recommend investigating paymentservice external dependency and adding circuit breaker"
    ]
  },
  {
    "name": "OTEL Demo: Ad Service Feature Flag Impact",
    "description": "Correlate adservice latency degradation with a feature flag change using traces and the featureflagservice",
    "category": "RCA",
    "difficulty": "Medium",
    "initialPrompt": "After a feature flag was toggled in the OpenTelemetry demo, the adservice response times increased from 50ms to 2 seconds. This is making every page load slow since ads load on every page. Investigate the traces in otel-v1-apm-span-* to understand how the feature flag change affected adservice performance.",
    "context": [
      {
        "description": "Ad Service Details",
        "value": "adservice (Java):\n- Operation: GetAds — returns contextual ads for products\n- Called by frontend on every page load\n- Checks feature flags via featureflagservice.GetFlag\n- Feature flag 'adServiceHighCpu' controls ad generation algorithm\n- Traces in otel-v1-apm-span-* with serviceName='adservice'"
      },
      {
        "description": "Feature Flag Spans",
        "value": "featureflagservice spans in traces:\n- serviceName: featureflagservice\n- span.attributes.feature_flag.key: adServiceHighCpu\n- span.attributes.feature_flag.variant: on/off\n- The flag was toggled from 'off' to 'on' approximately 1 hour ago\n- featureflagservice itself responds in <5ms (not the bottleneck)"
      },
      {
        "description": "Performance Comparison",
        "value": "Before flag toggle (flag=off):\n- adservice.GetAds: p50=50ms, p99=100ms\n- Single lightweight query per request\n\nAfter flag toggle (flag=on):\n- adservice.GetAds: p50=2000ms, p99=3500ms\n- Spans show CPU-intensive computation (ad relevance scoring)\n- No errors — just slow responses\n- Frontend total page load: baseline 300ms → now 2500ms"
      }
    ],
    "expectedOutcomes": [
      "Query traces to show adservice latency increase over time (before/after comparison)",
      "Find featureflagservice spans showing the adServiceHighCpu flag was toggled to 'on'",
      "Correlate the timestamp of the flag change with the start of latency degradation",
      "Identify that the flag enables a CPU-intensive ad relevance algorithm",
      "Recommend toggling adServiceHighCpu flag back to 'off' or optimizing the algorithm"
    ]
  },
  {
    "name": "OTEL Demo: Shipping Quote Inconsistency Across Currencies",
    "description": "Debug incorrect shipping quotes caused by currencyservice conversion errors visible in OTEL demo traces",
    "category": "RCA",
    "difficulty": "Hard",
    "initialPrompt": "Customers of the Astronomy Shop using non-USD currencies report shipping quotes that are 10x higher than expected. For example, a domestic shipment shows €500 instead of ~€5. The issue only affects certain currencies. Analyze the distributed traces in otel-v1-apm-span-* to trace the shipping quote calculation and find where the currency conversion goes wrong.",
    "context": [
      {
        "description": "Shipping Quote Flow",
        "value": "Shipping quote calculation path:\n1. frontend → checkoutservice.PlaceOrder\n2. checkoutservice → shippingservice.GetQuote (returns USD amount)\n3. checkoutservice → currencyservice.Convert (USD → user's currency)\n4. Result displayed to user on checkout page\n\nAlternate path for shipping estimate:\n- frontend → shippingservice.GetQuote (direct call, also needs conversion)"
      },
      {
        "description": "Currency Service Traces",
        "value": "currencyservice (C++) spans:\n- serviceName: currencyservice\n- span.attributes.rpc.method: Convert, GetSupportedCurrencies\n- Custom attributes in spans:\n  - app.currency.source: source currency code\n  - app.currency.destination: target currency code\n  - app.currency.amount.input: input amount in nanos\n  - app.currency.amount.output: converted amount in nanos\n- Currency amounts use Money proto: {units: int64, nanos: int32}"
      },
      {
        "description": "Observed Anomaly",
        "value": "From trace data:\n- USD to EUR conversions: input=599 units, output=55372 units (should be ~539)\n- USD to GBP conversions: input=599 units, output=47108 units (should be ~472)\n- USD to JPY conversions: input=599 units, output=89850 units (correct!)\n- USD to CAD conversions: input=599 units, output=81436 units (should be ~814)\n- Pattern: currencies with rates < 1.0 are multiplied by 100x\n- JPY works because its rate is > 100"
      }
    ],
    "expectedOutcomes": [
      "Query traces for currencyservice.Convert spans and extract currency conversion attributes",
      "Compare input and output amounts across different currency pairs",
      "Identify the pattern: currencies with exchange rate < 1.0 produce 100x inflated outputs",
      "Find evidence in the nanos field handling — likely a units vs nanos confusion in the conversion",
      "Determine root cause: currencyservice treats nanos as units for sub-1.0 exchange rates",
      "Recommend fixing the nanos/units handling in currencyservice conversion logic"
    ]
  }
]
